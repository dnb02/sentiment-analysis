{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LBa1XWlbmoJi",
        "F_GHR-TJmuUA",
        "3MG7heBMsDBS",
        "FypgbAgDvWcJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#April Doing"
      ],
      "metadata": {
        "id": "CNhWvWLYAt0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assume df is loaded\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Prepare the Data\n",
        "# Load and preprocess your sentiment classification dataset\n",
        "# Assuming you have X_train and y_train as your training data and labels\n",
        "\n",
        "# Step 2: Prepare Features and Labels\n",
        "# Extract features from your preprocessed data\n",
        "# Step 3: Split the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train a Gradient Boosting Model\n",
        "# Instantiate an XGBoost classifier with desired hyperparameters\n",
        "gbm = xgb.XGBClassifier(\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=100,\n",
        "    max_depth=3,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model to the training data\n",
        "gbm.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the Model\n",
        "# Predict on the test data\n",
        "y_pred = gbm.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Step 6: Fine-tune and Optimize\n",
        "# Experiment with different hyperparameter settings, feature representations, etc.\n",
        "# Repeat the process of training, evaluating, and fine-tuning until desired performance is achieved\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ft9XDl0Axer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Assuming 'X' and 'y' are your feature matrix and target vector, respectively\n",
        "dtrain = xgb.DMatrix(data=X, label=y, enable_categorical=True)\n"
      ],
      "metadata": {
        "id": "nv99u_U1DEO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df['review']\n",
        "y_train = df['label']"
      ],
      "metadata": {
        "id": "Q20mririCLuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(df['review'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_vectorized, df['label'], test_size=0.2, random_state=42)\n",
        "gbm = xgb.XGBClassifier(\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=100,\n",
        "    max_depth=7,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "gbm.fit(X_train, y_train)\n",
        "y_pred = gbm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeaF34msFDzq",
        "outputId": "74d4ceba-869d-49ea-9181-4a014e1430c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and preprocess your sentiment classification dataset into a DataFrame (df)\n",
        "# Assuming you have df with 'review' as features and 'label' as labels\n",
        "\n",
        "# Extract features and labels from the DataFrame\n",
        "X = df['review']\n",
        "y = df['label']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Count vectorization\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Instantiate a base classifier (e.g. DecisionTreeClassifier) and an AdaBoost classifier\n",
        "base_classifier = DecisionTreeClassifier(max_depth=10) # You can adjust the hyperparameters of the base classifier\n",
        "ada_boost = AdaBoostClassifier(base_classifier, n_estimators=100, random_state=42) # You can adjust the hyperparameters of AdaBoost\n",
        "\n",
        "# Fit the model to the training data\n",
        "ada_boost.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = ada_boost.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Fine-tune and optimize the model as needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Y8n-YuGSsc",
        "outputId": "773ad467-df34-48cd-d476-a1c7a4ff9a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7285714285714285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1RcECl3F8dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring sklearn inbuilt datasets"
      ],
      "metadata": {
        "id": "LBa1XWlbmoJi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQix39KhjSbZ"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cancer=datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "PtlpaKDikJn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=cancer.data\n",
        "y=cancer.target\n"
      ],
      "metadata": {
        "id": "HWlZxbpykIyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(cancer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKW0kJunlGiG",
        "outputId": "fb10c30f-b205-4244-b832-3d8787967d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DESCR',\n",
              " 'data',\n",
              " 'data_module',\n",
              " 'feature_names',\n",
              " 'filename',\n",
              " 'frame',\n",
              " 'target',\n",
              " 'target_names']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features=cancer.feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLMdpfRHk-df",
        "outputId": "e5b06b8a-7a4f-40f1-adf7-bbd15f7aefe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# stratified/normal Kfold cross validation"
      ],
      "metadata": {
        "id": "F_GHR-TJmuUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "docs = ['this is the first document', 'this is the second document', 'third document is here', 'the last document']\n",
        "\n",
        "# Define the labels for each document\n",
        "labels = ['class 1', 'class 1', 'class 2', 'class 2']\n",
        "\n",
        "# Define the number of folds\n",
        "k = 4\n",
        "\n",
        "# Define the vectorizer to transform the documents into a bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Transform the documents into a bag-of-words representation\n",
        "X = vectorizer.fit_transform(docs)\n",
        "\n",
        "# Define the model\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Define the cross-validation method\n",
        "kfold = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "# Evaluate the model using k-fold cross-validation\n",
        "scores = cross_val_score(model, X, labels, cv=kfold)\n",
        "\n",
        "# Print the average performance across all folds\n",
        "print(\"Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(scores) * 100, np.std(scores) * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAb-2pKEwsod",
        "outputId": "9475f195-353b-47ab-9060-cd07a69bbdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.00% (+/- 50.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = scaler.fit_transform(r)\n",
        "\n",
        "# from sklearn import linear_model\n",
        "# lr=linear_model.LogisticRegression()\n",
        "# lst_accu_stratified=[]\n",
        "\n",
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# skf=StratifiedKFold(n_splits=10,shuffle=True,random_state=1)\n",
        "\n",
        "# for train_index,test_index in skf.split(x,y):\n",
        "#   x_train_fold,x_test_fold=x_scaled[train_index],x_scaled[test_index]\n",
        "#   y_train_fold,y_test_fold=y[train_index],y[test_index]\n",
        "#   lr.fit(x_train_fold,y_train_fold)\n",
        "#   lst_accu_stratified.append(lr.score(x_test_fold,y_test_fold))\n",
        "\n",
        "# from statistics import mean,stdev\n",
        "\n",
        "# print('List of possible accuracy:', lst_accu_stratified)\n",
        "# print('\\nMaximum Accuracy That can be obtained from this model is:',\tmax(lst_accu_stratified)*100, '%')\n",
        "# print('\\nMinimum Accuracy:',\tmin(lst_accu_stratified)*100, '%')\n",
        "# print('\\nOverall Accuracy:',\tmean(lst_accu_stratified)*100, '%')\n",
        "# print('\\nStandard Deviation is:', stdev(lst_accu_stratified))"
      ],
      "metadata": {
        "id": "p0ZdzAhewTiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skeletal Code"
      ],
      "metadata": {
        "id": "3MG7heBMsDBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Sentiment Data"
      ],
      "metadata": {
        "id": "BTdjqOqIs1zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You've gotten data but how will you label 'em??\n",
        "#use pos and neg folder classification, and define a lambda?"
      ],
      "metadata": {
        "id": "kHy21Ge7Aib1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_ascii_1(text):\n",
        "  return ''.join([i if ord(i) < 128 else ' ' for i in text])"
      ],
      "metadata": {
        "id": "FsZwnYMns_WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "path=\"/content/drive/MyDrive/ProjectFiles/reviewzz\"\n",
        "\n",
        "reviews_pos = os.listdir(path+\"/pos\")\n",
        "reviews_neg = os.listdir(path+\"/neg\")\n",
        "reviews=[]\n",
        "for i in reviews_pos:\n",
        "  reviews.append(i)\n",
        "for i in reviews_neg:\n",
        "  reviews.append(i)\n",
        "r=[]\n",
        "for i in reviews_pos:\n",
        "  f=open(path+'/pos/'+i,mode='rb')\n",
        "  no_ascii=remove_non_ascii_1(str(f.read()))\n",
        "  r.append((no_ascii,i))\n",
        "  f.close()\n",
        "for i in reviews_neg:\n",
        "  f=open(path+'/neg/'+i,mode='rb')\n",
        "  no_ascii=remove_non_ascii_1(str(f.read()))\n",
        "  r.append((no_ascii,i))\n",
        "  f.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm07Zqi1s6Eg",
        "outputId": "f99d69f8-302a-4076-e1aa-0993fec293aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(r)\n",
        "df['label']=df[1].apply(lambda x: 1 if x[0:3]=='pos' else 0)\n",
        "df.columns=['review','file_name','label']\n",
        "df= df.drop('file_name',axis=1)"
      ],
      "metadata": {
        "id": "zzwyBOKEtB40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tuning=[1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9,3.0]\n",
        "X_train,X_test,y_train,y_test=train_test_split(df.review,df.label,test_size=0.2,random_state=False)"
      ],
      "metadata": {
        "id": "XbHj8S7ytGpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "\n",
        "v = CountVectorizer()\n",
        "X_train = v.fit_transform(X_train).toarray()\n",
        "model=GaussianNB()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "X_test = v.transform(X_test).toarray()\n",
        "y_pred=model.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))\n"
      ],
      "metadata": {
        "id": "Aa6cuvzzyZyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = CountVectorizer()\n",
        "#X_train = v.fit_transform(X_train).toarray()\n",
        "model=MultinomialNB()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "#X_test = v.transform(X_test).toarray()\n",
        "y_pred=model.predict(X_test)\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zehqjVYV7tDU",
        "outputId": "c3707469-61d0-412f-ec2f-e735464fd793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.83      0.79       135\n",
            "           1       0.83      0.75      0.79       145\n",
            "\n",
            "    accuracy                           0.79       280\n",
            "   macro avg       0.79      0.79      0.79       280\n",
            "weighted avg       0.79      0.79      0.79       280\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v=CountVectorizer()\n",
        "model=SVC()\n",
        "model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "print(accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UCSf6mB82nf",
        "outputId": "0ec8320f-0fc5-4c0e-9185-d97f6f96cb26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7607142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning\n"
      ],
      "metadata": {
        "id": "FypgbAgDvWcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "cLM5D0jqxWvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_tuning=[]\n",
        "for i in tuning:\n",
        "  model=MultinomialNB(alpha=i)\n",
        "  model.fit(X_train,y_train)\n",
        "  y_pred=model.predict(X_test)\n",
        "  hyperparameter_tuning.append((i,accuracy_score(y_test,y_pred)))\n",
        "print(hyperparameter_tuning)"
      ],
      "metadata": {
        "id": "wH1V_T1cxcsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding an ANN in PyTorch"
      ],
      "metadata": {
        "id": "3nAcxoYNZ41n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "3vMBM0k6Z8-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.w00 = nn.Parameter(torch.tensor(1.7),requires_grad=False)\n",
        "    self.b00 = nn.Parameter(torch.tensor(-.85),requires_grad=False)\n",
        "    self.w01 = nn.Parameter(torch.tensor(-40.8),requires_grad=False)\n",
        "\n",
        "    self.w10 = nn.Parameter(torch.tensor(12.8),requires_grad=False)\n",
        "    self.b10 = nn.Parameter(torch.tensor(0.),requires_grad=False)\n",
        "    self.w11 = nn.Parameter(torch.tensor(2.7),requires_grad=False)\n",
        "\n",
        "    self.final_bias = nn.Parameter(torch.tensor(-16.),requires_grad=False)\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    input_to_top_relu = input * self.w00 + self.b00\n",
        "    top_relu_output = F.relu(input_to_top_relu)\n",
        "    scaled_top_relu_output = top_relu_output * self.w01\n",
        "\n",
        "    input_to_bottom_relu = input * self.w10 + self.b10\n",
        "    bottom_relu_ouput = F.relu(input_to_bottom_relu)\n",
        "    scaled_bottom_relu_output = bottom_relu_ouput * self.w11\n",
        "\n",
        "    input_to_final_relu = scaled_top_relu_output + scaled_bottom_relu_output + self.final_bias\n",
        "\n",
        "    output = F.relu(input_to_final_relu)\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "nJ0sPJdRa_3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_doses = torch.linspace(start=0, end=1, steps=11)"
      ],
      "metadata": {
        "id": "xbNvqE71vhd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BasicNN()\n",
        "output_values = model(input_doses)"
      ],
      "metadata": {
        "id": "phkThQSqvrf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(x=input_doses, y=output_values.detach())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "WCNdTIYow6hO",
        "outputId": "ebedc2da-b916-40ad-d016-3fd970e42d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicNN_train(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.w00 = nn.Parameter(torch.tensor(1.7),requires_grad=False)\n",
        "    self.b00 = nn.Parameter(torch.tensor(-.85),requires_grad=False)\n",
        "    self.w01 = nn.Parameter(torch.tensor(-40.8),requires_grad=False)\n",
        "\n",
        "    self.w10 = nn.Parameter(torch.tensor(12.8),requires_grad=False)\n",
        "    self.b10 = nn.Parameter(torch.tensor(0.),requires_grad=False)\n",
        "    self.w11 = nn.Parameter(torch.tensor(2.7),requires_grad=False)\n",
        "\n",
        "    self.final_bias = nn.Parameter(torch.tensor(0.),requires_grad=True)\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "    input_to_top_relu = input * self.w00 + self.b00\n",
        "    top_relu_output = F.relu(input_to_top_relu)\n",
        "    scaled_top_relu_output = top_relu_output * self.w01\n",
        "\n",
        "    input_to_bottom_relu = input * self.w10 + self.b10\n",
        "    bottom_relu_ouput = F.relu(input_to_bottom_relu)\n",
        "    scaled_bottom_relu_output = bottom_relu_ouput * self.w11\n",
        "\n",
        "    input_to_final_relu = scaled_top_relu_output + scaled_bottom_relu_output + self.final_bias\n",
        "\n",
        "    output = F.relu(input_to_final_relu)\n",
        "    return output"
      ],
      "metadata": {
        "id": "dDr7rwjUvxDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BasicNN_train()\n",
        "output_values = model(input_doses)"
      ],
      "metadata": {
        "id": "RGz4uuBOw04-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.lineplot(x=input_doses, y=output_values.detach())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "3LPVtJp0vvS1",
        "outputId": "83bd0459-909f-419b-e0ea-9d6833b417ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ?Training ANN"
      ],
      "metadata": {
        "id": "5LfR3bd1xPCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([0., 0.5, 1.])\n",
        "labels = torch.tensor([0., 1., 0.])"
      ],
      "metadata": {
        "id": "w1W-zEmWyDd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = SGD(model.parameters(), lr=0.1)\n",
        "print(\"Final Bias before optimization: \"+str(model.final_bias.data)+\"\\n\")\n",
        "for epoch in range(100):\n",
        "  total_loss=0\n",
        "  for iteration in range(len(inputs)):\n",
        "    input_i = inputs[iteration]\n",
        "    label_i = labels[iteration]\n",
        "\n",
        "    output_i = model(input_i)\n",
        "    loss = (output_i - label_i)**2\n",
        "    loss.backward()\n",
        "    total_loss += float(loss)\n",
        "  if(total_loss<0.0001):\n",
        "    print(\"num steps: \"+str(epoch))\n",
        "    break\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "  print(\"Step: \"+str(epoch)+\" Final_bias: \",str(model.final_bias.data)+\"\\n\")\n"
      ],
      "metadata": {
        "id": "DEdYPA16xS6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#New paper Testing(unfruitful)"
      ],
      "metadata": {
        "id": "ktVc7OrBFjn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yoonkim/CNN_sentence.git\n",
        "!pip3 install theano==0.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTn2UGuhFjJQ",
        "outputId": "062acbee-0fb9-40ed-bbab-4a3b361b25e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CNN_sentence' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: theano==0.7 in /usr/local/lib/python3.9/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.9/dist-packages (from theano==0.7) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.11 in /usr/local/lib/python3.9/dist-packages (from theano==0.7) (1.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Sample code for\n",
        "Convolutional Neural Networks for Sentence Classification\n",
        "http://arxiv.org/pdf/1408.5882v2.pdf\n",
        "\n",
        "Much of the code is modified from\n",
        "- deeplearning.net (for ConvNet classes)\n",
        "- https://github.com/mdenil/dropout (for dropout)\n",
        "- https://groups.google.com/forum/#!topic/pylearn-dev/3QbKtCumAW4 (for Adadelta)\n",
        "\"\"\"\n",
        "\n",
        "import numpy\n",
        "import theano.tensor.shared_randomstreams\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "from theano.tensor.signal import downsample\n",
        "from theano.tensor.nnet import conv\n",
        "\n",
        "def ReLU(x):\n",
        "    y = T.maximum(0.0, x)\n",
        "    return(y)\n",
        "def Sigmoid(x):\n",
        "    y = T.nnet.sigmoid(x)\n",
        "    return(y)\n",
        "def Tanh(x):\n",
        "    y = T.tanh(x)\n",
        "    return(y)\n",
        "def Iden(x):\n",
        "    y = x\n",
        "    return(y)\n",
        "\n",
        "class HiddenLayer(object):\n",
        "    \"\"\"\n",
        "    Class for HiddenLayer\n",
        "    \"\"\"\n",
        "    def __init__(self, rng, input, n_in, n_out, activation, W=None, b=None,\n",
        "                 use_bias=False):\n",
        "\n",
        "        self.input = input\n",
        "        self.activation = activation\n",
        "\n",
        "        if W is None:\n",
        "            if activation.func_name == \"ReLU\":\n",
        "                W_values = numpy.asarray(0.01 * rng.standard_normal(size=(n_in, n_out)), dtype=theano.config.floatX)\n",
        "            else:\n",
        "                W_values = numpy.asarray(rng.uniform(low=-numpy.sqrt(6. / (n_in + n_out)), high=numpy.sqrt(6. / (n_in + n_out)),\n",
        "                                                     size=(n_in, n_out)), dtype=theano.config.floatX)\n",
        "            W = theano.shared(value=W_values, name='W')\n",
        "        if b is None:\n",
        "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
        "            b = theano.shared(value=b_values, name='b')\n",
        "\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        if use_bias:\n",
        "            lin_output = T.dot(input, self.W) + self.b\n",
        "        else:\n",
        "            lin_output = T.dot(input, self.W)\n",
        "\n",
        "        self.output = (lin_output if activation is None else activation(lin_output))\n",
        "\n",
        "        # parameters of the model\n",
        "        if use_bias:\n",
        "            self.params = [self.W, self.b]\n",
        "        else:\n",
        "            self.params = [self.W]\n",
        "\n",
        "def _dropout_from_layer(rng, layer, p):\n",
        "    \"\"\"p is the probablity of dropping a unit\n",
        "\"\"\"\n",
        "    srng = theano.tensor.shared_randomstreams.RandomStreams(rng.randint(999999))\n",
        "    # p=1-p because 1's indicate keep and p is prob of dropping\n",
        "    mask = srng.binomial(n=1, p=1-p, size=layer.shape)\n",
        "    # The cast is important because\n",
        "    # int * float32 = float64 which pulls things off the gpu\n",
        "    output = layer * T.cast(mask, theano.config.floatX)\n",
        "    return output\n",
        "\n",
        "class DropoutHiddenLayer(HiddenLayer):\n",
        "    def __init__(self, rng, input, n_in, n_out,\n",
        "                 activation, dropout_rate, use_bias, W=None, b=None):\n",
        "        super(DropoutHiddenLayer, self).__init__(\n",
        "                rng=rng, input=input, n_in=n_in, n_out=n_out, W=W, b=b,\n",
        "                activation=activation, use_bias=use_bias)\n",
        "\n",
        "        self.output = _dropout_from_layer(rng, self.output, p=dropout_rate)\n",
        "\n",
        "class MLPDropout(object):\n",
        "    \"\"\"A multilayer perceptron with dropout\"\"\"\n",
        "    def __init__(self,rng,input,layer_sizes,dropout_rates,activations,use_bias=True):\n",
        "\n",
        "        #rectified_linear_activation = lambda x: T.maximum(0.0, x)\n",
        "\n",
        "        # Set up all the hidden layers\n",
        "        self.weight_matrix_sizes = zip(layer_sizes, layer_sizes[1:])\n",
        "        self.layers = []\n",
        "        self.dropout_layers = []\n",
        "        self.activations = activations\n",
        "        next_layer_input = input\n",
        "        #first_layer = True\n",
        "        # dropout the input\n",
        "        next_dropout_layer_input = _dropout_from_layer(rng, input, p=dropout_rates[0])\n",
        "        layer_counter = 0\n",
        "        for n_in, n_out in self.weight_matrix_sizes[:-1]:\n",
        "            next_dropout_layer = DropoutHiddenLayer(rng=rng,\n",
        "                    input=next_dropout_layer_input,\n",
        "                    activation=activations[layer_counter],\n",
        "                    n_in=n_in, n_out=n_out, use_bias=use_bias,\n",
        "                    dropout_rate=dropout_rates[layer_counter])\n",
        "            self.dropout_layers.append(next_dropout_layer)\n",
        "            next_dropout_layer_input = next_dropout_layer.output\n",
        "\n",
        "            # Reuse the parameters from the dropout layer here, in a different\n",
        "            # path through the graph.\n",
        "            next_layer = HiddenLayer(rng=rng,\n",
        "                    input=next_layer_input,\n",
        "                    activation=activations[layer_counter],\n",
        "                    # scale the weight matrix W with (1-p)\n",
        "                    W=next_dropout_layer.W * (1 - dropout_rates[layer_counter]),\n",
        "                    b=next_dropout_layer.b,\n",
        "                    n_in=n_in, n_out=n_out,\n",
        "                    use_bias=use_bias)\n",
        "            self.layers.append(next_layer)\n",
        "            next_layer_input = next_layer.output\n",
        "            #first_layer = False\n",
        "            layer_counter += 1\n",
        "\n",
        "        # Set up the output layer\n",
        "        n_in, n_out = self.weight_matrix_sizes[-1]\n",
        "        dropout_output_layer = LogisticRegression(\n",
        "                input=next_dropout_layer_input,\n",
        "                n_in=n_in, n_out=n_out)\n",
        "        self.dropout_layers.append(dropout_output_layer)\n",
        "\n",
        "        # Again, reuse paramters in the dropout output.\n",
        "        output_layer = LogisticRegression(\n",
        "            input=next_layer_input,\n",
        "            # scale the weight matrix W with (1-p)\n",
        "            W=dropout_output_layer.W * (1 - dropout_rates[-1]),\n",
        "            b=dropout_output_layer.b,\n",
        "            n_in=n_in, n_out=n_out)\n",
        "        self.layers.append(output_layer)\n",
        "\n",
        "        # Use the negative log likelihood of the logistic regression layer as\n",
        "        # the objective.\n",
        "        self.dropout_negative_log_likelihood = self.dropout_layers[-1].negative_log_likelihood\n",
        "        self.dropout_errors = self.dropout_layers[-1].errors\n",
        "\n",
        "        self.negative_log_likelihood = self.layers[-1].negative_log_likelihood\n",
        "        self.errors = self.layers[-1].errors\n",
        "\n",
        "        # Grab all the parameters together.\n",
        "        self.params = [ param for layer in self.dropout_layers for param in layer.params ]\n",
        "\n",
        "    def predict(self, new_data):\n",
        "        next_layer_input = new_data\n",
        "        for i,layer in enumerate(self.layers):\n",
        "            if i<len(self.layers)-1:\n",
        "                next_layer_input = self.activations[i](T.dot(next_layer_input,layer.W) + layer.b)\n",
        "            else:\n",
        "                p_y_given_x = T.nnet.softmax(T.dot(next_layer_input, layer.W) + layer.b)\n",
        "        y_pred = T.argmax(p_y_given_x, axis=1)\n",
        "        return y_pred\n",
        "\n",
        "    def predict_p(self, new_data):\n",
        "        next_layer_input = new_data\n",
        "        for i,layer in enumerate(self.layers):\n",
        "            if i<len(self.layers)-1:\n",
        "                next_layer_input = self.activations[i](T.dot(next_layer_input,layer.W) + layer.b)\n",
        "            else:\n",
        "                p_y_given_x = T.nnet.softmax(T.dot(next_layer_input, layer.W) + layer.b)\n",
        "        return p_y_given_x\n",
        "\n",
        "class MLP(object):\n",
        "    \"\"\"Multi-Layer Perceptron Class\n",
        "\n",
        "    A multilayer perceptron is a feedforward artificial neural network model\n",
        "    that has one layer or more of hidden units and nonlinear activations.\n",
        "    Intermediate layers usually have as activation function tanh or the\n",
        "    sigmoid function (defined here by a ``HiddenLayer`` class)  while the\n",
        "    top layer is a softamx layer (defined here by a ``LogisticRegression``\n",
        "    class).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
        "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
        "\n",
        "        :type rng: numpy.random.RandomState\n",
        "        :param rng: a random number generator used to initialize weights\n",
        "\n",
        "        :type input: theano.tensor.TensorType\n",
        "        :param input: symbolic variable that describes the input of the\n",
        "        architecture (one minibatch)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: number of input units, the dimension of the space in\n",
        "        which the datapoints lie\n",
        "\n",
        "        :type n_hidden: int\n",
        "        :param n_hidden: number of hidden units\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of output units, the dimension of the space in\n",
        "        which the labels lie\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Since we are dealing with a one hidden layer MLP, this will translate\n",
        "        # into a HiddenLayer with a tanh activation function connected to the\n",
        "        # LogisticRegression layer; the activation function can be replaced by\n",
        "        # sigmoid or any other nonlinear function\n",
        "        self.hiddenLayer = HiddenLayer(rng=rng, input=input,\n",
        "                                       n_in=n_in, n_out=n_hidden,\n",
        "                                       activation=T.tanh)\n",
        "\n",
        "        # The logistic regression layer gets as input the hidden units\n",
        "        # of the hidden layer\n",
        "        self.logRegressionLayer = LogisticRegression(\n",
        "            input=self.hiddenLayer.output,\n",
        "            n_in=n_hidden,\n",
        "            n_out=n_out)\n",
        "\n",
        "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
        "        # be small\n",
        "\n",
        "        # negative log likelihood of the MLP is given by the negative\n",
        "        # log likelihood of the output of the model, computed in the\n",
        "        # logistic regression layer\n",
        "        self.negative_log_likelihood = self.logRegressionLayer.negative_log_likelihood\n",
        "        # same holds for the function computing the number of errors\n",
        "        self.errors = self.logRegressionLayer.errors\n",
        "\n",
        "        # the parameters of the model are the parameters of the two layer it is\n",
        "        # made out of\n",
        "        self.params = self.hiddenLayer.params + self.logRegressionLayer.params\n",
        "\n",
        "class LogisticRegression(object):\n",
        "    \"\"\"Multi-class Logistic Regression Class\n",
        "\n",
        "    The logistic regression is fully described by a weight matrix :math:`W`\n",
        "    and bias vector :math:`b`. Classification is done by projecting data\n",
        "    points onto a set of hyperplanes, the distance to which is used to\n",
        "    determine a class membership probability.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input, n_in, n_out, W=None, b=None):\n",
        "        \"\"\" Initialize the parameters of the logistic regression\n",
        "\n",
        "    :type input: theano.tensor.TensorType\n",
        "    :param input: symbolic variable that describes the input of the\n",
        "    architecture (one minibatch)\n",
        "\n",
        "    :type n_in: int\n",
        "    :param n_in: number of input units, the dimension of the space in\n",
        "    which the datapoints lie\n",
        "\n",
        "    :type n_out: int\n",
        "    :param n_out: number of output units, the dimension of the space in\n",
        "    which the labels lie\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
        "        if W is None:\n",
        "            self.W = theano.shared(\n",
        "                    value=numpy.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
        "                    name='W')\n",
        "        else:\n",
        "            self.W = W\n",
        "\n",
        "        # initialize the baises b as a vector of n_out 0s\n",
        "        if b is None:\n",
        "            self.b = theano.shared(\n",
        "                    value=numpy.zeros((n_out,), dtype=theano.config.floatX),\n",
        "                    name='b')\n",
        "        else:\n",
        "            self.b = b\n",
        "\n",
        "        # compute vector of class-membership probabilities in symbolic form\n",
        "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
        "\n",
        "        # compute prediction as class whose probability is maximal in\n",
        "        # symbolic form\n",
        "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
        "\n",
        "        # parameters of the model\n",
        "        self.params = [self.W, self.b]\n",
        "\n",
        "    def negative_log_likelihood(self, y):\n",
        "        \"\"\"Return the mean of the negative log-likelihood of the prediction\n",
        "        of this model under a given target distribution.\n",
        "\n",
        "    .. math::\n",
        "\n",
        "    \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) =\n",
        "    \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|} \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\\n",
        "    \\ell (\\theta=\\{W,b\\}, \\mathcal{D})\n",
        "\n",
        "    :type y: theano.tensor.TensorType\n",
        "    :param y: corresponds to a vector that gives for each example the\n",
        "    correct label\n",
        "\n",
        "    Note: we use the mean instead of the sum so that\n",
        "    the learning rate is less dependent on the batch size\n",
        "    \"\"\"\n",
        "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
        "        # number of examples (call it n) in the minibatch\n",
        "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
        "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
        "        # Log-Probabilities (call it LP) with one row per example and\n",
        "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
        "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
        "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
        "        # the mean (across minibatch examples) of the elements in v,\n",
        "        # i.e., the mean log-likelihood across the minibatch.\n",
        "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
        "\n",
        "    def errors(self, y):\n",
        "        \"\"\"Return a float representing the number of errors in the minibatch ;\n",
        "    zero one loss over the size of the minibatch\n",
        "\n",
        "    :type y: theano.tensor.TensorType\n",
        "    :param y: corresponds to a vector that gives for each example the\n",
        "    correct label\n",
        "    \"\"\"\n",
        "\n",
        "        # check if y has same dimension of y_pred\n",
        "        if y.ndim != self.y_pred.ndim:\n",
        "            raise TypeError('y should have the same shape as self.y_pred',\n",
        "                ('y', target.type, 'y_pred', self.y_pred.type))\n",
        "        # check if y is of the correct datatype\n",
        "        if y.dtype.startswith('int'):\n",
        "            # the T.neq operator returns a vector of 0s and 1s, where 1\n",
        "            # represents a mistake in prediction\n",
        "            return T.mean(T.neq(self.y_pred, y))\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "class LeNetConvPoolLayer(object):\n",
        "    \"\"\"Pool Layer of a convolutional network \"\"\"\n",
        "\n",
        "    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2), non_linear=\"tanh\"):\n",
        "        \"\"\"\n",
        "        Allocate a LeNetConvPoolLayer with shared variable internal parameters.\n",
        "\n",
        "        :type rng: numpy.random.RandomState\n",
        "        :param rng: a random number generator used to initialize weights\n",
        "\n",
        "        :type input: theano.tensor.dtensor4\n",
        "        :param input: symbolic image tensor, of shape image_shape\n",
        "\n",
        "        :type filter_shape: tuple or list of length 4\n",
        "        :param filter_shape: (number of filters, num input feature maps,\n",
        "                              filter height,filter width)\n",
        "\n",
        "        :type image_shape: tuple or list of length 4\n",
        "        :param image_shape: (batch size, num input feature maps,\n",
        "                             image height, image width)\n",
        "\n",
        "        :type poolsize: tuple or list of length 2\n",
        "        :param poolsize: the downsampling (pooling) factor (#rows,#cols)\n",
        "        \"\"\"\n",
        "\n",
        "        assert image_shape[1] == filter_shape[1]\n",
        "        self.input = input\n",
        "        self.filter_shape = filter_shape\n",
        "        self.image_shape = image_shape\n",
        "        self.poolsize = poolsize\n",
        "        self.non_linear = non_linear\n",
        "        # there are \"num input feature maps * filter height * filter width\"\n",
        "        # inputs to each hidden unit\n",
        "        fan_in = numpy.prod(filter_shape[1:])\n",
        "        # each unit in the lower layer receives a gradient from:\n",
        "        # \"num output feature maps * filter height * filter width\" /\n",
        "        #   pooling size\n",
        "        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) /numpy.prod(poolsize))\n",
        "        # initialize weights with random weights\n",
        "        if self.non_linear==\"none\" or self.non_linear==\"relu\":\n",
        "            self.W = theano.shared(numpy.asarray(rng.uniform(low=-0.01,high=0.01,size=filter_shape),\n",
        "                                                dtype=theano.config.floatX),borrow=True,name=\"W_conv\")\n",
        "        else:\n",
        "            W_bound = numpy.sqrt(6. / (fan_in + fan_out))\n",
        "            self.W = theano.shared(numpy.asarray(rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
        "                dtype=theano.config.floatX),borrow=True,name=\"W_conv\")\n",
        "        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX)\n",
        "        self.b = theano.shared(value=b_values, borrow=True, name=\"b_conv\")\n",
        "\n",
        "        # convolve input feature maps with filters\n",
        "        conv_out = conv.conv2d(input=input, filters=self.W,filter_shape=self.filter_shape, image_shape=self.image_shape)\n",
        "        if self.non_linear==\"tanh\":\n",
        "            conv_out_tanh = T.tanh(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
        "            self.output = downsample.max_pool_2d(input=conv_out_tanh, ds=self.poolsize, ignore_border=True)\n",
        "        elif self.non_linear==\"relu\":\n",
        "            conv_out_tanh = ReLU(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
        "            self.output = downsample.max_pool_2d(input=conv_out_tanh, ds=self.poolsize, ignore_border=True)\n",
        "        else:\n",
        "            pooled_out = downsample.max_pool_2d(input=conv_out, ds=self.poolsize, ignore_border=True)\n",
        "            self.output = pooled_out + self.b.dimshuffle('x', 0, 'x', 'x')\n",
        "        self.params = [self.W, self.b]\n",
        "\n",
        "    def predict(self, new_data, batch_size):\n",
        "        \"\"\"\n",
        "        predict for new data\n",
        "        \"\"\"\n",
        "        img_shape = (batch_size, 1, self.image_shape[2], self.image_shape[3])\n",
        "        conv_out = conv.conv2d(input=new_data, filters=self.W, filter_shape=self.filter_shape, image_shape=img_shape)\n",
        "        if self.non_linear==\"tanh\":\n",
        "            conv_out_tanh = T.tanh(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
        "            output = downsample.max_pool_2d(input=conv_out_tanh, ds=self.poolsize, ignore_border=True)\n",
        "        if self.non_linear==\"relu\":\n",
        "            conv_out_tanh = ReLU(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
        "            output = downsample.max_pool_2d(input=conv_out_tanh, ds=self.poolsize, ignore_border=True)\n",
        "        else:\n",
        "            pooled_out = downsample.max_pool_2d(input=conv_out, ds=self.poolsize, ignore_border=True)\n",
        "            output = pooled_out + self.b.dimshuffle('x', 0, 'x', 'x')\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "Q_XUKbn9GDu_",
        "outputId": "36be447b-f3c4-4372-aec4-69ce7c245f8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c186c0945f47>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_randomstreams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/theano/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_asarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_asarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoldr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/theano/printing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_graphviz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mpydot_imported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pydot' has no attribute 'find_graphviz'"
          ]
        }
      ]
    }
  ]
}